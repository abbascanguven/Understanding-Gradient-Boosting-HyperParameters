{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Import Sacred Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import shapiro\nfrom sklearn import preprocessing\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\")\ndf = data.copy()\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column Mean\n\n* age: The person's age in years\n* sex: The person's sex (1 = male, 0 = female)\n* cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n* trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n* chol: The person's cholesterol measurement in mg/dl\n* fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n* restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n* thalach: The person's maximum heart rate achieved\n* exang: Exercise induced angina (1 = yes; 0 = no)\n* oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n* slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n* ca: The number of major vessels (0-3)\n* thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n* target: Heart disease (0 = no, 1 = yes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sex'][df['sex'] == 0] = 'female'\ndf['sex'][df['sex'] == 1] = 'male'\n\ndf['cp'][df['cp'] == 0] = 'typical angina'\ndf['cp'][df['cp'] == 1] = 'atypical angina'\ndf['cp'][df['cp'] == 2] = 'non-anginal pain'\ndf['cp'][df['cp'] == 3] = 'asymptomatic'\n\ndf['trestbps'][df['trestbps'] == 0] = 'lower than 120mg/ml'\ndf['trestbps'][df['trestbps'] == 1] = 'greater than 120mg/ml'\n\ndf['restecg'][df['restecg'] == 0] = 'normal'\ndf['restecg'][df['restecg'] == 1] = 'ST-T wave abnormality'\ndf['restecg'][df['restecg'] == 2] = 'left ventricular hypertrophy'\n\ndf['exang'][df['exang'] == 0] = 'no'\ndf['exang'][df['exang'] == 1] = 'yes'\n\ndf['slope'][df['slope'] == 0] = 'upsloping'\ndf['slope'][df['slope'] == 1] = 'flat'\ndf['slope'][df['slope'] == 2] = 'downsloping'\n\ndf['thal'][df['thal'] == 1] = 'normal'\ndf['thal'][df['thal'] == 2] = 'fixed defect'\ndf['thal'][df['thal'] == 3] = 'reversable defect'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['thal'][df['thal'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([48, 281], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Missing_Values(data):\n    variable_name=[]\n    total_value=[]\n    total_missing_value=[]\n    missing_value_rate=[]\n    unique_value_list=[]\n    total_unique_value=[]\n    data_type=[]\n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()/data[col].shape[0],3))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n    missing_data=pd.DataFrame({\"Variable\":variable_name,\"Total_Value\":total_value,\\\n                             \"Total_Missing_Value\":total_missing_value,\"Missing_Value_Rate\":missing_value_rate,\n                             \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value})\n    return missing_data.sort_values(\"Missing_Value_Rate\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Missing_Values(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test split"},{"metadata":{},"cell_type":"markdown","source":"prepare get data "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummie = pd.get_dummies(df)\ndf_dummie","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_dummie.drop(\"target\", axis = 1)\ny = df_dummie[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nprint(X_train.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = GradientBoostingClassifier()\ngb_model = gb.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n\nac = accuracy_score(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"ac = {}\\nclass_report =\\n{}\".format(ac, class_report))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Hyperparameters Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb.get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* loss{‘deviance’, ‘exponential’}, default= ’deviance’\n\n*The loss function to be optimized. ‘deviance’ refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss ‘exponential’  gradient boosting recovers the AdaBoost algorithm.*\n\n* learning_rate, float, default=0.1\n\n*A technique to slow down the learning in the gradient boosting model is to apply a weighting factor for the corrections by new trees when added to the model. This weighting is called the shrinkage factor or the learning rate, depending on the literature or the tool.*\n\n* n_estimators, int, default=100\n\n*The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.*\n\n* subsample, float, default=1.0\n\n*The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.*\n\n* criterion{‘friedman_mse’, ‘mse’, ‘mae’}, default=’friedman_mse’\n\n*The function to measure the quality of a split. Supported criteria are ‘friedman_mse’ for the mean squared error with improvement score by Friedman, ‘mse’ for mean squared error, and ‘mae’ for the mean absolute error. The default value of ‘friedman_mse’ is generally the best as it can provide a better approximation in some cases.*\n\n* min_samples_split, int or float, default=2\n\n*The min_samples_split parameter will evaluate the number of samples in the node, and if the number is less than the minimum the split will be avoided and the node will be a leaf.*\n\n* min_samples_leaf, int or float, default=1\n\n*The min_samples_leaf parameter checks before the node is generated, that is, if the possible split results in a child with fewer samples, the split will be avoided (since the minimum number of samples for the child to be a leaf has not been reached) and the node will be replaced by a leaf.*\n\n* min_weight_fraction_leaf, float, default=0.0\n\n*The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.*\n\n* max_depth, int, default=3\n\n*The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.*\n\n* min_impurity_decrease, float, default=0.0\n\n*The min impurity decrease function formula can be found here. The formula is defined as:\n\n*N_t / N * (impurity - N_t_R / N_t * right_impurity\n                - N_t_L / N_t * left_impurity)*\n*where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.*\n\n*N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.*\n\n* min_impurity_split, float, default=None\n\n*Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.*\n\n* random_state, int, RandomState instance or None, default=None\n\n*Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random spliting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls.*\n\n* max_features{‘auto’, ‘sqrt’, ‘log2’}, int or float, default=None\n\n*Auto/None : This will simply take all the features which make sense in every tree.Here we simply do not put any restrictions on the individual tree.*\n\n*sqrt : This option will take square root of the total number of features in individual run. For instance, if the total number of variables are 100, we can only take 10 of them in individual tree.*\n\n*log2: is another similar type of option for max_features.*\n\n*0.2 : This option allows the GradientBoosting to take 20% of variables in individual run. We can assign and value in a format “0.x” where we want x% of features to be considered.\n\n* verbose, int, default=0\n\n*Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree.*\n\n* max_leaf_nodes, int, default=None\n\n*Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.*\n\n* warm_start,bool, default=False\n\n*When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution.*\n\n*When fitting an estimator repeatedly on the same dataset, but for multiple parameter values (such as to find the value maximizing performance as in grid search), it may be possible to reuse aspects of the model learned from the previous parameter value, saving time. When warm_start is true, the existing fitted model attributes are used to initialize the new model in a subsequent call to fit. Note that this is only applicable for some models and some parameters, and even some orders of parameter values. For example, warm_start may be used when building random forests to add more trees to the forest (increasing n_estimators) but not to reduce their number. partial_fit also retains the model between calls, but differs: with warm_start the parameters change and the data is (more-or-less) constant across calls to fit; with partial_fit, the mini-batch of data changes and model parameters stay fixed. There are cases where you want to use warm_start to fit on different, but closely related data. For example, one may initially fit to a subset of the data, then fine-tune the parameter search on the full dataset. For classification, all data in a sequence of warm_start calls to fit must include samples from each class.\n\n* validation_fraction, float, default=0.1\n\n*The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if n_iter_no_change is set to an integer.*\n\n* n_iter_no_change, int, default=None\n\n*n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. The split is stratified.*\n\n* tol, float, default= 0.0001\n\nTolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations (if set to a number), the training stops.\n\n\n* ccp_alphanon-negative, float, default=0.0\n\n*Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details.*\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's do the tuned process for an example\n\ngb_params = {'ccp_alpha': [0.0, 0.1],\n 'criterion': ['friedman_mse', 'mse', 'mae'],\n 'learning_rate': [0.1, 0.3],\n 'loss': ['deviance', 'exponential'],\n 'max_depth': [3, 5],\n 'max_features': [\"auto\" ,\"sqrt\"],\n 'max_leaf_nodes': [5, 10],\n 'min_impurity_decrease': [0.0],\n 'min_impurity_split': [2,5],\n 'min_samples_leaf': [1, 5],\n 'min_samples_split': [2, 6],\n 'min_weight_fraction_leaf': [0.0],\n 'n_estimators': [50, 100, 200],\n 'n_iter_no_change': [0],\n 'presort': ['deprecated'],\n 'random_state': [42],\n 'subsample': [1.0, 0.8],\n 'tol': [0.0001], \n 'validation_fraction': [0.1],\n 'verbose': [0],\n 'warm_start': [False]}\n\n\n\ngb_tuned = GridSearchCV(gb, gb_params, cv = 5, verbose = 2, \n                      n_jobs = -1)\ngb_tuned = gb_tuned.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_tuned.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = GradientBoostingClassifier()\ngb_model = gb.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n\nac = accuracy_score(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"ac = {}\\nclass_report =\\n{}\".format(ac, class_report))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = GradientBoostingClassifier(ccp_alpha= 0.0,\n criterion= 'friedman_mse',\n learning_rate= 0.1,\n loss= 'deviance',\n max_depth= 3,\n max_features= 'auto',\n max_leaf_nodes= 5,\n min_impurity_decrease= 0.0,\n min_impurity_split= 2,\n min_samples_leaf= 1,\n min_samples_split= 2,\n min_weight_fraction_leaf= 0.0,\n n_estimators= 50,\n n_iter_no_change= 0,\n presort= 'deprecated',\n random_state= 42,\n subsample= 1.0,\n tol= 0.0001,\n validation_fraction= 0.1,\n verbose= 0,\n warm_start= False)\n\ngb_model = gb.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n\nac = accuracy_score(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"ac = {}\\nclass_report =\\n{}\".format(ac, class_report))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is seen as a result of our tuned process. The result is pretty bad. This is because I don't fill in the parameters with the correct values. I wrote this notebook to learn the meaning of hyper parameters. I hope it works for you"},{"metadata":{},"cell_type":"markdown","source":"### Woks Cited\n\n\n[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n\n[scikit-learn 2](https://scikit-learn.org/stable/glossary.html#term-warm_start)\n\n[stackoverflow](https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre#:~:text=min_samples_split%20specifies%20the%20minimum%20number,then%20the%20split%20is%20allowed.)\n\n[stackoverflow 2](https://stackoverflow.com/questions/54812230/sklearn-min-impurity-decrease-explanation)\n\n[analyticsvidhya](https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/#:~:text=max_features%3A,Python%20to%20assign%20maximum%20features.)\n\n[Kaggle](https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model#The-Data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}